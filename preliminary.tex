%
% The teaching materials for the Hadoop MapReduce module are grouped into four categories: lecture notes and example codes, assignments, data sources, and tools to set up Hadoop platforms \cite{CPSC:3620}. The lectures and assignments on MapReduce and HDFS with the guiding theme of data intensive computing familiarize the students with concepts of knowledge units in the area of Parallel and Distributed Computing as defined by the ACM/IEEE curricula \cite{acm:ieee:curricula}, as shown in Table \ref{tab:curricula}.
% \begin{table*}
% \caption{Parallel and Distributed Computing Learning Outcomes through Hadoop MapReduce lectures and assignments.}
% \label{tab:curricula}
% \centering
% \begin{tabular}{|l|l|l|l|}
% \hline
% Level & Knowledge Area & Knowledge Unit & Learning Outcome \\
% \hline
% Familiarity & \begin{tabular}{@{}l@{}}Parallel \& Distributed \\ Computing \end{tabular} & \begin{tabular}{@{}l@{}} Parallelism \\ Fundamentals \end{tabular} & \begin{tabular}{@{}l@{}}Distinguishing using computational \\ resources for a faster answer from managing \\ efficient access to a shared resource \end{tabular} \\
% \hline
% Familiarity & \begin{tabular}{@{}l@{}}Parallel \& Distributed \\ Computing \end{tabular} & \begin{tabular}{@{}l@{}} Parallel \\ Architecture \end{tabular} & \begin{tabular}{@{}l@{}} Describe the key performance challenges in different \\ memory and distributed system topologies \end{tabular} \\
% \hline
% Usage & \begin{tabular}{@{}l@{}}Parallel \& Distributed \\ Computing \end{tabular} & \begin{tabular}{@{}l@{}} Parallel \\ Performance \end{tabular} & \begin{tabular}{@{}l@{}} Explain performance impacts of data locality \end{tabular} \\
% \hline
% Familiarity & \begin{tabular}{@{}l@{}}Information \\ Management \end{tabular} & \begin{tabular}{@{}l@{}} Distributed \\ Databases \end{tabular} & \begin{tabular}{@{}l@{}} Explain the techniques used for data \\ fragmentation, replication, and allocation \\ during the distributed database design process \end{tabular} \\
% \hline
% Assessment & \begin{tabular}{@{}l@{}}Parallel \& Distributed \\ Computing \end{tabular} & \begin{tabular}{@{}l@{}} Parallel Algorithms, \\ Analysis, and Programming \end{tabular} & \begin{tabular}{@{}l@{}} Decompose a problem via \\ map and reduce operations \end{tabular} \\
% \hline
% Usage & \begin{tabular}{@{}l@{}}Parallel \& Distributed \\ Computing \end{tabular} & \begin{tabular}{@{}l@{}} Parallel \\ Performance \end{tabular} & \begin{tabular}{@{}l@{}} Observe how data distribution/layout can \\ affect an algorithm's communication costs \end{tabular} \\
% \hline
% \end{tabular}
% \end{table*}
%
%
% \subsection{Lecture Notes and Example Codes}
%
% For the Hadoop MapReduce module, we have lectures on the three topics: MapReduce, HDFS, and HBase. There exists a large number of reference lectures from academic and industry sources for these topics. We follow the set of lecture notes from \cite{lin:2008}. However, we enhance the MapReduce/HDFS lectures with additional visualizations, for example, Figure \ref{fig:hdfs},
% that demonstrate the HDFS-MapReduce integration and how this integration lies on top of the physical computing cluster and the Linux file system. The first coding example accompanying the MapReduce lecture is the standard WordCount example which illustrates the basic concepts of mapping and reducing. This is followed by another WordCount example that uses the reducer as a combiner. The students observe the tradeoff between increased map task run time (observed through Hadoop's JobTracker's web interface) versus reduced network traffic (observed through final MapReduce job report). In the MapReduce lab, the students goes through another set of MapReduce examples analyzing the Airline Traffic data. The analysis is to find out the average delay time for each individual airline on the entire data set. Three examples of code are provided which implement different algorithmic choices described in \cite{lin:2013}. These examples emphasize the usage of MapReduce's combiner, the customized MapReduce's Value classes, and the trade-off in memory and network traffic due to different implementations of the combiner. The first example code is a standard MapReduce program whose mappers emit the airline code and the delay time as a key-value pair and reducers calculate the average delay time for all the values of the same key. The second example code implements a combiner, which also requires the implementation of a customized Hadoop Value class. The third example code utilizes global memory on each node to implement a combining mechanism without implementing a combiner class.
%
% \begin{figure*}
% 	\begin{center}
% 	{\scalebox{0.85}{\includegraphics{hdfs_mapreduce.pdf}}}
%   \setlength{\belowcaptionskip}{-10pt}
% 	\caption{Illustration to emphasize the relationship between components of the base Hadoop ecosystem and the underlying hardware and the Linux file system}
% 	\label{fig:hdfs}
% 	\end{center}
% \end{figure*}
%
% %\subsection{Example Codes}
% %
% %The example codes are more relevant toward the MapReduce segment of the module. The first example codes accompanying the MapReduce lecture are the standard WordCount example and a WordCount example that calls its reducer as its combiner. In the MapReduce lab the class goes through another set of MapReduce examples analyzing airline traffic data. The analytical example asks to find out the average delay time for each individual airline on the entire data set. Three example codes are provided. The first example code is a standard MapReduce with the mapper emits the airline code and the delay time as a key-value pair, and the reduce calculate the average delay time for all the values of the same key. The second example code implements a combiner, which also requires the implementation of a customized Hadoop Value class. The third example code utilizes global memory on each node to implement a combining mechanism without implementing a combiner.
%
% \subsection{Assignments}
%
% Assuming that the students are familiar with the basics of writing code in a Linux environment, the process of compiling and executing a simple MapReduce program is automated through a scripting template for MapReduce without HDFS. The first assignment requires the student to recall the lab materials by implementing a number of descriptive statistics calculations on the rating of individual movie genres from the movie rating database \cite{movielens}. While the calculations are relatively straightforward, the matching of the ratings for individual movies into the relevant genres that the movies belong to requires the map tasks to interact with an additional data file. As the students later learned, the optimized implementation of this external access with respect to the map tasks can make the program run one order of magnitude faster. The easiest, but inefficient approach, is to read the additional file from inside each mapper. An alternative and more efficient approach is to implement a Java object that reads the additional file once and stores the content in memory. The mappers can then access this object as needed. The second part of the assignment requires the students to implement a single MapReduce program to identify the user that provides the most ratings and that user's favorite movie genre. In order to answer this question, the students need to implement a customized Hadoop output value class, as the information needed in the reduce step requires several values for each key. The first assignment has the students run their final jars using only serial Java commands without any HDFS support.
%
% The second assignment familiarizes student with the underlying HDFS and how it supports the MapReduce programming model. The first part of this assignment takes the jar files from the first assignment and reruns them on the data on HDFS. The goal of this part is to demonstrate the ease in which Hadoop MapReduce can immediately speed up the application without having to worry about parallel workload division, process' ranks, etc. This part also requires students to execute and record the output of a number of Hadoop shell commands to observe how HDFS transforms, stores, replicates, and abstracts the actual data to support data intensive computing. The second part of this assignment asks the students to analyze the Yahoo song database (10GB) \cite{yahoomusic} and identify the album that has the highest average rating using MapReduce and HDFS. Again, this requires the students to access the list of songs in each album to support the main rating data files.
%
% In both assignments, while the students can follow different approaches in designing the MapReduce programs in order to find the answers to the problems, the differences between runtimes of efficient and inefficient approaches can be observed. Having individual mappers reading from the same additional data file increases runtimes to several hours, and implementing a customized Java object to preprocess the additional data can reduce the runtimes to minutes.
%
% \subsection{Data}
%
% To make the assignments realistic, the data set needs to have some aspects that are ``big'' as compared to a typical data table. Utilizing full text data such as Wikipedia is a convenient way to have a big data set. However, without getting into complex text analysis methods, full text analysis is limited to counting words and recognizing phrases. The Google Trace data \cite{clusterdata:Wilkes2011} is large enough and contains complex and interesting information to be analyzed. However, as the size of the Google Trace data is relatively large (171GB), it can take over an hour for students to stage the data into the temporary Hadoop cluster. This trace data is more appropriate for semester projects rather than weekly assignments. The Airline Traffic data \cite{airtraffic} has a reasonable size (12GB) with a straightforward single-table data schematic. We utilize the Airline Traffic data in our examples so that we can spend more time on the programming techniques and less time for data movement.
%
% The first assignment uses the Movie Rating dataset \cite{movielens}, which is 250MB in size and contains 10 million ratings for 10,000 movies by 72,000 users. This provides the students with a significant number of records on a more complex data structure that can still be processed serially within a reasonable amount of time. The results from the students' assignments show that the best implementation of the first assignment can run as fast as several minutes, while the worst implementation takes a little over half an hour to run. The second assignment uses the Yahoo Music Rating dataset \cite{yahoomusic}. This dataset is similar in size to the Airline Traffic data with a complex set of tables that is similar to the Movie Rating dataset. The data is large enough to be impractical on a serial execution yet small enough so that it takes less than five minutes to load the data into the HDFS file system.
%
% \subsection{Tools for setting up the Hadoop platform}
%
% To ensure consistent configurations for the Hadoop platforms, regardless of whether they are set up on the supercomputer or on personal machines, we provided students with a package of Java 1.7.0\_25 and Apache Hadoop 1.2.1. The package is decompressed into students' Linux home directories. The students only needed to modify their \textit{.bashrc} to source a script that automatically identifies the paths for JAVA\_HOME and HADOOP\_HOME. Another script is provided with  commands to clean up temporary output directories, to compile and compress MapReduce source codes into \textit{jar} files, and to execute these \textit{jar} files in serial mode on the Linux file system without HDFS support.
%
% For the dynamic Hadoop platform on the supercomputer with HDFS, the myHadoop scripts were modified so that the only changes the students needed to make were changes to the Hadoop platform's physical configurations (number of nodes, size of RAM, and expected run time) on the scheduler's submission script. The submission script also includes Hadoop commands to automatically create HDFS directories, load data from the Linux file system, check HDFS' health status, execute an example MapReduce job, and export output data back to students' home directories on the supercomputer's Linux file system. With a batch submission, the scheduler will record all outputs from these commands, so that the students can review and analyze the performance of their Hadoop platforms. As the large scale parallel storage of Clemson's supercomputer is not configured with file-locking support, we cannot use the persistent storage option of myHadoop, and all Hadoop data storage must reside on the local hard drive of the scheduled compute nodes. In order to analyze MapReduce output file on HDFS in a batch execution, the students must export the desired HDFS data to the Linux file system by placing either \textit{hadoop fs -copyToLocal} or \textit{hadoop fs -get} after the command to execute the MapReduce job. The students can also insert a \textit{sleep} command into the submission script and turn the dynamic Hadoop platform into an interactive platform for the duration of the \textit{sleep} command.
